{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIB: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**\n",
    "* $x$ be our input source,\n",
    "* $y$ be our target\n",
    "* $z$ be our latent representation\n",
    "\n",
    "### Mutual Information\n",
    "Mutual information (MI) measures the amount of information obtained about one random variable after observing another random variable. Formally given two random variables $x$ and $y$ with joint distribution $p(x,y)$ and marginal densities $p(x)$ and $p(y)$ their MI is defined as the KL-divergence between the joint density and the product of their marginal densities\n",
    "$$\\begin{align}\n",
    "I(x;y)&=I(y;x)\\\\\n",
    "&=KL\\Big(p(x,y)||p(x)p(y)\\Big)\\\\\n",
    "&=\\mathbb{E}_{(x,y)\\sim p(x,y)}\\bigg[\\log\\frac{p(x,y)}{p(x)p(y)}\\bigg]\\\\\n",
    "&=\\int dxdyp(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{align}$$\n",
    "\n",
    "### Information Bottlenecks\n",
    "IB regards supervised learning as a representation learning problem, seeking a stochastic map from input data\n",
    "$x$ to some latent representation $z$ that can still be used to predict the labels $y$ , under a constraint on its total complexity.\n",
    "\n",
    "We assume our joint distribution $p(x,y,z)$ can be factorised as follows:\n",
    "$$p(x,y,z)=p(z\\mid x,y)p(y\\mid x)p(x)=p(z\\mid x)p(y\\mid x)p(x)$$\n",
    "which corresponds to the following Markov Chain \n",
    "$$y\\rightarrow x\\rightarrow z$$\n",
    "\n",
    "Our goal is to learn an encoding that is maximally informative about our target $y$ measured by $I(y;z)$. We could always ensure a maximally informative representation by taking the identity encoding $x=z$ which is not useful. Instead we apply a constraint such that the objective is\n",
    "$$\\begin{alignat}{3}\n",
    "    &\\underset{}{\\text{max }} & \\quad & I(y;z)\\\\\n",
    "    &\\text{subject to } & \\quad & I(x;z)\\leq I_c\n",
    "\\end{alignat}$$\n",
    "where $I_c$ is the information constraint. The Lagrangian of the above constrained optimisation problem is\n",
    "$$\\begin{align}\n",
    "    \\mathcal{L}_{IB}&=\\text{max }I(y;z)-\\beta \\big(I(x;z)-I_c\\big)\\\\\n",
    "    &=\\text{max }I(y;z)-\\beta I(x;z)\n",
    "\\end{align}$$\n",
    "where $\\beta\\geq0$ is a Lagrange multiplier. \n",
    "* Intuitively the first term encourages $z$ to be predictive of $y$, whilst the second term encourages $z$ to \"forget\" $x$. \n",
    "* In essence, IB principle explicitly enforces the learned representation $z$ to only preserve the information in $x$ that is useful to the prediction of $y$, i.e., the minimal sufficient statistics of $x$ w.r.t. $y$.\n",
    "\n",
    "### Variational Information Bottlenecks\n",
    "\n",
    "**The first term**<br>\n",
    "We can write out the terms in the objective as\n",
    "$$I(y;z)=\\int dydz p(y,z)\\log \\frac{p(y,z)}{p(y)p(z)}=\\int dydz p(y,z)\\log \\frac{p(y\\mid z)}{p(y)}$$\n",
    "where $p(y\\mid z)$ is defined as\n",
    "$$p(y\\mid z)=\\int dx \\frac{p(x,y,z)}{p(z)}=\\int dx \\frac{p(z\\mid x)p(y\\mid x)p(x)}{p(z)}$$\n",
    "which is intractable. Let $q(y\\mid z)$ be a variational approximation to $p(y\\mid z)$. By using the KL divergence we can obtain a lower bound on $I(y;z)$\n",
    "$$KL\\Big(p(y\\mid z)|| q(y\\mid z)\\Big)\\geq0\\Longrightarrow \\int dy p(y\\mid z)\\log p(y\\mid z)\\geq \\int dy p(y\\mid z)\\log q(y\\mid z)$$\n",
    "Hence we have that \n",
    "$$\\begin{align}\n",
    "    I(y;z)&= \\int dydz p(y,z)\\log p(y\\mid z) - \\int dy p(y)\\log p(y)\\\\\n",
    "    &\\geq \\int dydz p(y, z)\\log q(y\\mid z) - \\int dy p(y)\\log p(y)\\\\\n",
    "    &=\\int dxdydz p(z\\mid x)p(y\\mid x)p(x)\\log q(y\\mid z)\n",
    "\\end{align}$$\n",
    "where the entropy of the labels $H(y)=- \\int dy p(y)\\log p(y)$ is independent of our optimisation and so can be ignored.\n",
    "\n",
    "\n",
    "**The second term**<br>\n",
    "We can write out the second term in the objective as \n",
    "$$I(x;z)=\\int dxdz p(x,z)\\log \\frac{p(x,z)}{p(x)p(z)}=\\int dxdz p(x,z)\\log \\frac{p(z\\mid x)}{p(z)}$$\n",
    "Let $q(z)$ be a variational approximation to the marginal $p(z)$. By using the KL divergence we can obtain an upper bound on $I(x;z)$ as \n",
    "$$KL\\Big(p(z)|| q(z)\\Big)\\geq0\\Longrightarrow \\int dz p(z)\\log p(z)\\geq \\int dz p(z)\\log q(z)$$\n",
    "Hence we have\n",
    "$$\\begin{align}\n",
    "    I(x;z)&=\\int dxdz p(x,z)\\log p(z\\mid x) - \\int dz p(z)\\log p(z)\\\\\n",
    "    &\\leq\\int dxdz p(x,z)\\log p(z\\mid x) - \\int dz p(z)\\log q(z)\\\\\n",
    "    &=\\int dxdz p(x)p(z\\mid x)\\log \\frac{p(z\\mid x)}{q(z)}\n",
    "\\end{align}$$\n",
    "\n",
    "### Loss Function\n",
    "Combining the above two bounds we have that \n",
    "$$I(y;z)-\\beta I(x;z)\\geq \\int dxdydz p(z\\mid x)p(y\\mid x)p(x)\\log q(y\\mid z) -\\beta\\int dxdz p(x)p(z\\mid x)\\log \\frac{p(z\\mid x)}{q(z)} = L$$\n",
    "\n",
    "To compute the lower bound in practice make the following assumptions:\n",
    "\n",
    "* We approximate $p(x,y)=p(x)p(y\\mid x)$ using the empirical data distribution $p(x,y)=\\frac{1}{n}\\sum^{n}_{i=1}\\delta_{x_i}(x)\\delta_{y_i}(y)$ such that \n",
    "$$\\begin{align}\n",
    "L&\\approx \\frac{1}{n}\\sum^{n}_{i=1}\\bigg[\\int dz p(z\\mid x_i)\\log q(y_i\\mid z)-\\beta\\int dz p(z\\mid x_i)\\log \\frac{p(z\\mid x_i)}{q(z)}\\bigg]\\\\\n",
    "&=\\frac{1}{n}\\sum^{n}_{i=1}\\bigg[\\int dz p(z\\mid x_i)\\log q(y_i\\mid z)- \\beta KL\\Big(p(z\\mid x_i)|| q(z)\\Big) \\bigg]\n",
    "\\end{align}$$\n",
    "\n",
    "* By using an encoder parameterised as multivariate Gaussian\n",
    "$$p_\\phi(z\\mid x)=\\mathcal{N}\\bigg(z;\\boldsymbol{\\mu}_\\phi(x), \\boldsymbol{\\Sigma}_\\phi(x)\\bigg)$$\n",
    "then we can use the reparameterisation trick such that $z=g_\\phi(\\epsilon,x)$ which is a deterministic function of $x$ and the Gaussian random variable $\\epsilon\\sim p(\\epsilon)=\\mathcal{N}(0,I)$.\n",
    "\n",
    "* We assume that our choice of parameterisation of $p(z\\mid x)$ and $q(z)$ allow for computation of an analytic KL-divergence, \n",
    "\n",
    "Thus the final objective we would **minimise** is \n",
    "$$J_{IB}=\\frac{1}{n}\\sum^{n}_{i=1}\\Bigg[\\beta KL\\Big(p(z\\mid x_i)|| q(z)\\Big) - \\mathbb{E}_{\\epsilon\\sim p(\\epsilon)}\\Big[\\log q\\big(y_i\\mid g_\\phi(\\epsilon,x)\\big)\\Big]\\Bigg]$$\n",
    "where we have that \n",
    "* $p_\\phi(z\\mid x)$ is the encoder parameterised as a multivariate Gaussian \n",
    "$$p_\\phi(z\\mid x)=\\mathcal{N}\\bigg(z;\\boldsymbol{\\mu}_\\phi(x), \\boldsymbol{\\Sigma}_\\phi(x)\\bigg)$$\n",
    "* $q_\\theta(y\\mid z)$ is the decoder parameterised as an independent Bernoulli for each element $y_j$ of $y$ (for binary data) \n",
    "$$q_\\theta(y_j\\mid z)=\\text{Ber}\\Big(\\mu_\\theta(z)\\Big)$$\n",
    "* $q(z)$ is the approximated latent marginal often fixed to a standard normal.\n",
    "$$q_\\theta(z)=\\mathcal{N}\\Big(z;\\mathbf{0},\\mathbf{I}_k\\Big)$$\n",
    "\n",
    "**Unsure about this**<br>\n",
    "By using our parameterisation of the decoder $q_\\theta(y\\mid z)$ as an indepenedent Bernoulli we have that \n",
    "$$-\\log q_\\theta(y\\mid z)=-y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})$$\n",
    "i.e. this is the Binary Cross Entropy loss.\n",
    "\n",
    "### Connection to Variational Autoencoder\n",
    "The VAE is a special case of an unsupervised version of VIB with $\\beta=1.0$ as they consider the objective \n",
    "$$L=I(x;z)-\\beta I(i;z)$$\n",
    "where the aim is to take our data $x$ and maximize the mutual information contained in some encoding $z$, while restricting how much information we allow our representation to contain about the identity of each data element in our sample $i$. While this objective takes the same mathematical form as that of a Variational Autoencoder, the interpretation of the objective is very different:\n",
    "* In the VAE, the model starts life as a generative model with a defined prior $p(z)$ and stochastic decoder $p(x|z)$ as part of the model, and the encoder $q(z|x)$ is created to serve as a variational approximation to the true posterior $p(z|x) = \\frac{p(x|z)}{p(z)p(x)}$. \n",
    "* In the VIB approach, the model is originally just the stochastic encoder $p(z|x)$, and the decoder $q(x|z)$ is the variational approximation to the true $p(x|z) = \\frac{p(z|x)p(x)}{p(z)}$ and $q(z)$ is the variational approximation to the marginal $p(z) =\\int dx p(x)p(z|x)$.\n",
    "\n",
    "### References\n",
    "* Original Deep VIB paper: https://arxiv.org/abs/1612.00410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIB: Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is almost identical to my VAE implementation found here: [torch_vae](https://github.com/udeepam/vae/blob/master/notebooks/vae.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# Device Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "seed = 73\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torchvision.datasets import MNIST\n",
    "# # 60000 tuples with 1x28x28 image and corresponding label\n",
    "# data = MNIST('data', \n",
    "#              train=True, \n",
    "#              download=True,\n",
    "#              transform = transforms.Compose([transforms.ToTensor()]))\n",
    "# # Split data into images and labels\n",
    "# x_train = data.train_data\n",
    "# y_train = data.train_labels\n",
    "# # Scale images from [0,255] to [0,+1]\n",
    "# x_train = x_train.float() / 255\n",
    "# # Save as .npz\n",
    "# np.savez_compressed('data/mnist_train', \n",
    "#                     a=x_train,\n",
    "#                     b=y_train)\n",
    "\n",
    "# # 10000 tuples with 1x28x28 image and corresponding label\n",
    "# data = MNIST('data', \n",
    "#              train=False, \n",
    "#              download=True,\n",
    "#              transform = transforms.Compose([transforms.ToTensor()]))\n",
    "# # Split data into images and labels\n",
    "# x_test = data.test_data\n",
    "# y_test = data.test_labels\n",
    "# # Scale images from [0,255] to [0,+1]\n",
    "# x_test = x_test.float() / 255\n",
    "# # Save as .npz\n",
    "# np.savez_compressed('data/mnist_test', \n",
    "#                     a=x_test,\n",
    "#                     b=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data locally\n",
    "train_data = np.load('data/mnist_train.npz')\n",
    "x_train = torch.Tensor(train_data['a'])\n",
    "y_train = torch.Tensor(train_data['b'])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "test_data = np.load('data/mnist_test.npz')\n",
    "x_test = torch.Tensor(test_data['a'])\n",
    "y_test = torch.Tensor(test_data['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAB8oAAAHhCAYAAAAGQjRqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdebxtdV0//tf7DgwXBQUDNVGcEMFSFBKcwDTTNI1yqMxEM79lTqlp+mvA1NQyc8LMAXFIM7UU09RQIFMGcUoRNBUcURABJ8DLvev3x943D4dz7j33s/c5+577eT4fj/U496693uvz2cM5r33Oe+21ahiGAAAAAAAAAEAv1sx6AgAAAAAAAACwkjTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJTTlao6uqqGqjpu1nMBAJaPzAeAPsh8AOiH3AemTaO8E1V1wDhAtracOut5rnYLPKZXVNW3q+q0qvrrqrrNlMY5cbz/A6axv0XG2PKaeddyjcHSVdVxW/ne/eGs5wfsOGT+ypD5LKeq2q2q/qqqvlxVV1XV16vqZVV1vVnPDdhxyPyVIfNZKVV186r64fg5esms5wPsWOT+ypD7LJequuX4b/zvqapvjZ+bT896XiTrZj0BVtwXkvzzIrddsILz2Jl9J8mrxv/eJcnPJDk8yTOTPKOq/i7JM4ZhGGY0P1a3N+Ta36s/mcE8gB2fzF9+Mp+pq6q1Sf49yb2SfDTJ25PcNskTkhxdVXcdhuEHM5wisOOR+ctP5rOsqqqSvG7W8wBWBbm//OQ+y+HuSf4yydVJzk1yo9lOhy00yvtz3jAMx816Eju5by/0GFfVkUnenORPkmxM8v+t8LzYOZw4DMOps54EsCrI/OUn81kOj8qoSf6mJI/c8seXqnpGkhckeXqSP5/d9IAdkMxffjKf5faHSe6RUc7/3YznAuzY5P7yk/ssh9OSHJHkM8MwXFlVDrTYQTj1OguqqhPGp3543Lz1VVUfGN/2q3PW36mqjq+qc6rq+1X1o6r6ZFX90fio2Pn7H6rq1Kq6SVW9raq+V1WXV9Xbq2q/8TZHVNWHq+oHVXVxVb24qtbN28+x430dW1UPGY95xfjUFX9XVXtsx30+dDz+d8anuPxyVT2/qq6z/Y/gtQ3DcHqSX05yZZKnVdX+c8beq6r+tKo+Mj6Vy0+q6mtV9Y9VdcN587wgySPH/z1/zmlgTpyzzaOr6qSq+ur4vny3qt5dVYdNej/mnBbmFlX1jPHjdEVVfbqq7jveZs/x6+HC8W0fXui0NFV1zPj5/8p4u0ur6uSquvciY1+nqv5+/PxeMX6+HzL3dbBAzT2r6n1VdUlVXVlVnx8/1s0HCtXotDWbapHT1owfm81V9e7WMQBWisyX+YuR+TPL/MckGZI8a94nFF6c5KIkj17oew1gW2S+zF+MzJ/t7/lVdbMkL0zyoiSfnPb+gT7Jfbm/GLk/m9wfhuH8YRjOHIbhymntk+nQKGcxT0zy5SQvqqrbzln/5CT3SfKqYRjeM2f97yd5UJLPZHRakjcm2SvJK5L8/SJjXD/JR5LcOMnrk3wiyYOTvLuq7pLkQ0m+l+TVSS5O8sdZ/Cith2T0qZvPJnlpkguTPCXJe6pqm6/zqjomyRlJ7pvkg0leltGpav40yclVtcu29rEUwzB8KcnbMjplyzFzbrptkmcn+WGSd4zvw3lJHpvk9Kq6/pxtX5LR45zxds8eL3N/qB+f0SlhPpjR4/+fGT1v/11VR0zjvoz3+4TxGG9OcmCSk6rq8Iyeu7tkdBqg9ye5Z5J/r9GpROd6fpKDMjqa6iVJ3p3ksCQfqKpfn7vhuPZ9Gb0Gvzne/n8yeq09eKEJVtXjx3M5bLzv45P8YDzuv7Te8WEYLsjoMb3//Dc6Y49OslynTbt7VT29qp5aVb9SVbsuwxhAX2S+zN8Wmb9CmV9Vu2d0Sr/zhmH4xry5bExySkbfR7eexnhAd2S+zN8Wmb/Cv+dX/d8p1y9Mctw09w10T+7L/W2R+7P5+z47mmEYLB0sSQ7I6JMp52X0xnuh5Yh5NXfO6BQin8roB//PZXS01HlJNszb9qZJ1sxbty6jH6Kbktxs3m3DePmbeetPGq+/NMn956zfI6NwvCTJ+jnrj52zr6PmrF+TUbAMSR49Z/3R43XHzVl3gyTfT/KVJDeeN58/GW//tCU+zkOST29jm0eNt3vjnHV7Jdl7gW1/Z7ztn81bf+J4/QGLjHHzBdbdNqMQOXk7XzPvWmTsc5PsM2f9b8x57t6aZO2c214+vu3XlzDP/ZJ8I8mX5q3//fE+/iVJzVl/9ySbx7cdO2f9IePX7xlJ9pqzvjJ6gzckefAE31Nb7u/T561fk+Tr49frunmvveO2Yzl63n6Py09f63OXbyb5xdb7YbFYdr4lMn/L+qMj82X+Ksv8JLcbj3XSInN5zvj2X2m9PxaLZedZIvO3rD86Ml/mr7LMn7OPPxjf13vMez2/pPV+WCyWnXOJ3N+yfsvPyePmrJP7W3/NyP2FH58Vz/3tfa1ZVmaZ+QQsK/RE//SH4taWJy9Q9xfj216W0dFcP0lyx+0Y99fn/4Abrx/GP9TnB/KW4PjQAvt67fi2m89Zd+x43fsX2P7g8W0fnrPu6Fw7SJ+y2A/V8Q/Fi5KcvcT7u5Qgve94u/ctYX+V5LIkp85bf2K2EqRb2d9JSa5Ksst2vGYWC9JHLPBYXTW+bf95t91tvP7ZS5zny+bfvySnjtcduMD275v/Opuzj8MW2H7PjML3HRN8T61P8p2MPvE1d/39xuO+YN7645bwPTh3OW5e/a8l+d0kN0uyW5JbJfmzJD8eL4e03heLxbJzLZH5W9YdPf/naWT+tl4zMn/hOa5Y5md0xP6Q5M2LzGXLH3l+q/X+WCyWnWeJzN+y7ugFfp7K/K2/ZmT+wnNc6d/zb5pRY+eVC7yeNcotFss1lsj9Leu2/Jw8bs46ub/114zcX3iOK5r7La81y8oszefwZ9V69zAMv7Yd2z8vo1N6PGH8/z8dhuFa10san/75iUkeluQ2SeZf9+NGC+z7f4dh+PG8dd8ef/3M/I3n3HbjJOfPu+2/5288DMPnq+rSJLdfYF9z3Xn89W5VdbsFbt+Y0elDpmXBa0pW1b0yOu3ILyTZJ8nc05gs9PgtPkDVrZI8K6NTotw4oyMG59onoyOiJnGN52gYhs1VdXFGb46+Pm/buc/d3HneMMkzMwqf/TNqAM91o4xOkZOMnsdLhmH44gJzOX28j7nunFHYPLCqHrBAzRWZ4HkdhmHj+LoxT6+quw7D8NHxTb83/nrCvO2PywSnURuGYf71Ur6U5LlV9Z2MTl/0p0ke0bp/YKck869N5reR+SuX+VteM0NjPdAnmX9tMr+NzF/B3/MzahhdluQZE+wD6I/cvza530bur2zus4PSKGerhmHYVFXvTXLXjI4mWuyaDO9Mcv+MTtvyloyuOXJ1RkctPTLJQtdR/v4C665ewm3rF7jt4kXm9Z2MPnm7NXuPvz5pG9tNy5ZQ/L85V9VDM7rexw8yOp3NBRn9oE9G4brk61BX1a2TnJXkuklOTvJvGV0bZXNGn0q+/fbsbysWe46W9NxV1d7jed4kozdC/5Hk8vE8j05y1Lx5XjfJ/y4yl4sWWLd3Rm9a/nyxO5DRKX8m8dqMPtn1e0k+WlU3SPKrSf5rkcBfDm9I8sqMvkcBmsn8ZSHzI/O30+Xjr3stcvue87YD2G4yf1nI/Mj87VFVxyb5pYxOS/yDae0XYD65vyzkfuQ+Ow+Ncraqqg7J6PQsl2R0lNIrkzx03jaHZxSi78/oDf7mObc9LKMgXW4/s8j6/bLwD/a5ttx+62EYvjS9KS3qqPHXs+es+8uMgvOOwzB8ecvKqqokT9/O/T85yfWS/PYwDG+de0NV3TnbPgJvpfxeRkeZPWsYhufPvaGq/iE/fZy2+EEWf573XWDd9zO6fs4ewzBcNeFcFzQMw/9W1WlJHlpVT8zoE927ZIE3nFV1dEZvEJbq1GEYTl3CHH5SVT9IsmE79g1wLTJ/Wcj8EZm/dXMz/8sZ/VFhsT8E3Xr8dSVev8BOSuYvC5k/IvO3bm7m32H89b2jl8S1PKmqnpTkDcMwHLsdYwBcg9xfFnJ/RO5v3ZL+vs/saZSzqPHpVt6S0SlCfjnJU5P8VlUdOwzDiXM2veX463vnhujYSn3K9W7zV1TVwUmun+SUbdSeldG1Vo7IMv/RcXzKlIdmdC2YuafSvmWSz80N0bFDk+y+wK42jb+uXeC2Lc/He+aNvXuSO27vnJfRYvOsJEcusP1nkhxVVQcucDTXQtufldH9PTwLnLpnil6TUUA+LMmjMwrwdyyw3dEZvWHaHqdua4Pxa+r6Sc7czn0D/B+ZP30y/xpk/radmiTDMFxRVR9P8gtVdZNhGL6xZYOqWp/RafcuzOJH4QNslcyfPpl/DTJ/204dfz091z6tcTL6lOKvJDknyRlJPrrANgBLIvenT+5fg9zftlMnmBcrZM2sJ8AO7flJfj7JccMwfCLJHyb5WpKXVdUt5mz3tfHXa4RmVR2R5LErMdEkv1xV/3eEUlWtyej6K0ny5m3Uvj6jU5e8cHxak2uoqutV1aGTTnD8eHwgo+t0/O2863x8Lcmtq2rfOdvvmeSli+zue+OvP7vAbdd6Psbh9NdZ+MisWVnwdZPREXMLHRW35ei559Scw62r6m5J7rvA9q/M6A3H8VV1rWvAVNV+VXXbeesuqKqhqg5Y0j0YeWdGR2T+VZLbJXnrAtfmyTAMxw3DUNuxHDdnXrtU1WEL3IfrZRTkyejUPgCtZH5k/jKS+UvM/LHXZXR6ub+ee/+TPCWj5/WEYRhcwxxoJfMj85eRzF9i5g/D8LZhGB4zf0nyt+NNTh6ve/12zBtgPrkfub+M5P72/a7PDsonyvtzUFUdt8htlw3D8JIkqapfyugH2n8neUGSDMNweVU9IqMjuP6pqu4+DMPVGX2S9ewkv1lVN0zy8SS3SPLAJCcl+Y1lvD9bvC/JB6rqbUm+leQ+GR1tdGqSE7dWOAzDRVX18CRvS3JOVb0vo0/p7JHR/Tgqo+tA/8ES53LDOY/x+oxOJ3J4RqfV2pzRLz3zr6vxioxC85NV9c6Mrt1xv/F9+dYCY5yS5GlJXlVV70jy4ySfHYbhvUleleRRSf51/HhcnuTuSW6e0eNx9BLvx3J7U5JnJHlFVd0zyTeSHJbRkX/vzeh0P3O9LsnvZnTE3s2r6kMZHWn9sIye//tn9PgmSYZh+GxVPSGjx/aL4+f1goyOQrx1Rkcp/nmSc+eMseXgoauzRMMwXFVVb8ro+yUZXddk2jYk+XhVfTrJpzO6Ns+NM3qN3CCj0yIdvwzjAqubzJ9H5s+MzN8+J2R0Xx+R5BZV9ZEkt03yoCSfS/LCZRoXWL1k/jwyf2ZkPsDyk/vzyP2ZkfvboUbXP3/RvNU3raoTx//+7jAMT1uOsdmGYRgsHSxJDkgybGO5YLztPkm+mdEP4AMW2Nfzx9s/e866/TIKrG9l9EP9E0kentEP7SGjo9bm7mPI6BoN8/e94Pbj244b33b0nHXHjtcdm+QhST6V0bVALkzy4oyuX7HU/R88vg9fz+jUKd9N8smM3kgctMTHef5jemWSbyc5LaMjvg5cpK6S/FGSz4/n//WMgvW6Gf3wv2CBmmdmdB3LjeOxTpxz272SfCyj635cktFRUbce379hoed1K6+Zd81bv+g+tjLXA+bPcbz+0CT/meTS8evtAxm96bjWcz3e/rrjx+XC8eP0qfHz/tTx9scsMPaRSd4+rvnJ+Pk4I6Nr89x0znZ7ZXSE2n83fH/dcTz+Z5bp+3fX8f0+M8lF4+f8siQfSfL/kqxZjnEtFsvqXCLzl7J/mb/wa0bmb/uxWtbMnzPO7kmek+QrSa7K6A8OL09yveUc12KxrK4lMn8p+5f5C79mZP62H6sVyfytfL+8ZCXHtVgsO/4Sub+U/cv9hV8zcn/bj9Wy5362/T18rcfdsjJLjZ8gWJWq6tiMTq3yqOGa11WhI+Mjvn4nySHDMHy+cR+/kvGRbsMwvG87ax+d0RFxTxqG4WUt4wOwdTKfROYD9EDmk8h8gF7IfRK5z2y5RjmwaixyLZK7JfnNjE6nc+61ipbubhkdMba9Ibo2yRMzOtLyTROMDwCMyXwA6IPMB4B+yH12RK5RDqwmr6mqGyc5K8n3kxyUn1675InDBKfIGIbhWUmetdTtq+rnkvxqRteHuX2SFw7DcGnr+ADANch8AOiDzAeAfsh9djga5cBq8i8ZXZP7wRldc+TyJO9L8vxhGD62wnO5U5LnZXT9lddmdN0VAGA6ZD4A9EHmA0A/5D47HNcoBwAAAAAAAKArO9QnynepXYfdssespwEA13BlfpSfDFfVrOexM5H5AOyofpBLvzsMw8/Meh47E7kPwI7I7/rTJ/MB2FEt9rv+DtUo3y175M51r1lPAwCu4czhQ7Oewk5H5gOwozp5eMdXZz2HnY3cB2BH5Hf96ZP5AOyoFvtdf81KTwQAAAAAAAAAZmmqjfKq2r+q3lFVl1fV96vqX6vqptMcAwDYMch9AOiDzAeAfsh9AHoytUZ5VW1I8uEkByV5ZJJHJLl1klOqyoVJAGAnIvcBoA8yHwD6IfcB6M00r1H++0lukeQ2wzB8KUmq6n+S/G+S/5fkxVMcCwCYLbkPAH2Q+QDQD7kPQFemeer1ByY5Y0uAJskwDOcn+WiSB01xHABg9uQ+APRB5gNAP+Q+AF2ZZqP8kCSfW2D9OUkOnuI4AMDsyX0A6IPMB4B+yH0AujLNU6/vneTSBdZ/L8n1FyuqqscmeWyS7JYNU5wOALCMtjv3ZT4ArEp+1weAfvhdH4CuTPMT5UkyLLCutlowDK8ehuGwYRgOW59dpzwdAGAZbVfuy3wAWLX8rg8A/fC7PgDdmGaj/NKMjjib7/pZ+Cg0AGD1kvsA0AeZDwD9kPsAdGWajfJzMrqGyXwHJ/n8FMcBAGZP7gNAH2Q+APRD7gPQlWk2yk9KckRV3WLLiqo6IMldx7cBADsPuQ8AfZD5ANAPuQ9AV6bZKH9NkguSvLuqHlRVD0zy7iRfT/KPUxwHAJg9uQ8AfZD5ANAPuQ9AV6bWKB+G4UdJfjHJF5O8Kck/JTk/yS8Ow/DDaY0DAMye3AeAPsh8AOiH3AegN+umubNhGL6W5DemuU8AYMck9wGgDzIfAPoh9wHoyTRPvQ4AAAAAAAAAOzyNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAuqJRDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAuqJRDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXVk36wkAAACzc/Uv3qm59sLHXdVc+5kj39Bce/vTH9lce+Pjd2muTZK1p3xyonoAAAAAdgw+UQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArqyb9QRgZ1fr2r/N1v7MDaY4k5Xzhacd0Fy7acPm5tqb3fKi5toNj6vm2iT59ot3aa795GFva6797qYfNdfe+e1Pba691VPOaK4FYLo2H3XoRPUvO+EVzbW3Wt/+Pqc98ZNPHfn65tovHLZpgpGTPzngiInqAYDV40cPvnNz7Qv/5h+aa5/z0N9trh3O/lxzLQDM0pf/9sjm2nN/u/1vG0myvtY2197jcY9trt39XWc11zIdPlEOAAAAAAAAQFc0ygEAAAAAAADoylQb5VV1dFUNCyyXTXMcAGC2ZD4A9EPuA0AfZD4AvVmua5Q/McnH5/z/6mUaBwCYLZkPAP2Q+wDQB5kPQBeWq1F+7jAMZyzTvgGAHYfMB4B+yH0A6IPMB6ALrlEOAAAAAAAAQFeWq1H+T1W1qaouqaq3VNVNl2kcAGC2ZD4A9EPuA0AfZD4AXZj2qdcvT/J3SU5L8v0khyZ5VpLTq+rQYRguml9QVY9N8tgk2S0bpjwdAGCZyHwA6IfcB4A+yHwAujLVRvkwDJ9K8qk5q06rqv9KclaSJyb5swVqXp3k1UmyZ+09THM+AMDykPkA0A+5DwB9kPkA9GbZr1E+DMMnk3wxyeHLPRYAMDsyHwD6IfcBoA8yH4Cd2bI3yscqiaPJAGDnJ/MBoB9yHwD6IPMB2Ckte6O8qg5LcmCSM5d7LABgdmQ+APRD7gNAH2Q+ADuzqV6jvKr+Kcn5ST6Z5LIkhyZ5ZpJvJnn5NMcCAGZH5gNAP+Q+APRB5gPQm6k2ypN8LslvJXlCkg1Jvp3kX5P85TAM353yWADA7Mh8AOiH3AeAPsh8ALoy1Ub5MAzPT/L8ae6Tncva2956ovph1/XNtd866nrNtVcc8aPm2r33aq/9yO3f1lzbo//48XWba1/4ivtONPaZP/eW5trzN17RXPuC7/xSc+2NP+LSUrST+TBdG+9zWHPt01/5ponGPnD9Ls21m7O5ufYrGzc2116+edfm2kPbS5MkV93v8Oba3U/5bHPt5iuvbK6FScn97XfFg36hvXaftc21e59wenMtcG0XHdZ+1crnXPCrU5wJrAyZD0zq2398l+baUx/2N821G4f2v21MzJ/ZV7Vlv0Y5AAAAAAAAAOxINMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICurJv1BFh9Nh19x+baF594/ERjH7h+l4nq2fFtHDY11/7Fy49trl33o6G5NkmOfPvjm2uv+82rm2t3/e4VzbUbzj6zuRZgZ7V2zz2ba390j4Oaa//479/SXHvP3X/YXDsym2NnT7z0Ls21H3rlkc21Hz3uZc21SfKfr31Vc+3Bb25/v3CLZ5zeXAusvG/do/1n64ZbXtY+8AntpbBTWrN2ovLhpu2/c99r3/Oaaz9U7e+TAGCWfrj/5ubavdfo/7DyfKIcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF1ZN+sJsPrs+oVvNdd+4sr9Jxr7wPXfmai+J0+98IiJ6r/ywxs01554y3c0116+eWiu3e9lH2uuXa3aHy0AFvKNN/5sc+3HDz9+ijPZ+f3Vvh9vrn3/de7SXPuoC+7TXJskbzjg5ObaPQ++ZKKxgdXj2Q94e3PtC8+d7OcU8FNrb3mzierPO+qE5to7nPU7zbU3/vhnm2sBYFI/fMidm2vfecxLJxi5mitfddlBE4ybnPzQw5pr9/jqOc21m5srmRafKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAuqJRDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOjKullPgNXn6gu/3Vz78hc+ZKKxn3ffHzXXrv2f6zTXfuZxL2+uncRzv/vzzbVfuveGicbedNmFzbW/feTjmmsveGJzaW6ez7QXA7DTuPoX79Rc+9Y7vKK5dk12aa6dxKO+eq+J6s8++bbNtZ/9vfbH65Qrdmuu3ffsK5prv3TpQc21SbL+r09prl1TEw0NrCLr6+pZTwFIsu61P57Z2Fd8ec+ZjQ0AVz7gF5pr//L5JzTXHrh+Nr/4vuE1952o/oaf/9iUZsJq4xPlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAuqJRDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOjKullPgL7s/frTJ6r/mffs01y76ZLvNdcecrtHN9eec48TmmtPevVRzbX7Xvax5tpJ1emfaa69+WQvEQB2EpuPOrS59mUnvKK59lbr298eb87m5toHnndMc+3aB/+ouTZJrnf/obn24Dc9vrn2wOO/3ly75uufaq69/keaS5MkG5+3qbn2nT/f/r7w0fd8YnPt2sV8cE0AACAASURBVFM+2VwLPdt8tzs01959t/+e4kyAVgfsccnMxt7/5Pb3DAAwqQt/58rm2nvu3l6brG2ufOQF926uveFLZ9cPYXXziXIAAAAAAAAAurKkRnlV3aSqXl5Vp1fVj6tqqKoDFthut6r626q6sKquGG9/j2lPGgBYHjIfAPoh9wGgDzIfABa21E+U3yrJQ5NcmmRrJyt8XZLfT/IXSR6Q5MIkH6iq9vOVAQArSeYDQD/kPgD0QeYDwAKWehHG/xqGYb8kqarHJLnP/A2q6vZJfjvJo4dheP143WlJzknyV0keOJUZAwDLSeYDQD/kPgD0QeYDwAKW9InyYRg2L2GzBybZmORtc+quTvLPSX65qnZtmiEAsGJkPgD0Q+4DQB9kPgAsbKmnXl+KQ5KcPwzDj+etPyfJLhmd3gUAWP1kPgD0Q+4DQB9kPgDdWeqp15di74yucTLf9+bcfi1V9dgkj02S3bJhitMBAJaJzAeAfsh9AOiDzAegO9P8RHklGRZZv6hhGF49DMNhwzActj7O3gIAq4DMB4B+yH0A6IPMB6A702yUfy8LH1V2/Tm3AwCrn8wHgH7IfQDog8wHoDvTbJSfk+TmVTX//CoHJ/lJki9NcSwAYHZkPgD0Q+4DQB9kPgDdmWaj/KQk65M8ZMuKqlqX5GFJPjgMw1VTHAsAmB2ZDwD9kPsA0AeZD0B31i11w6p68Pifdxp/vV9VXZzk4mEYThuG4dNV9bYkL6mq9UnOT/KHSW6e5OHTnDQAsHxkPgD0Q+4DQB9kPgBc25Ib5UnePu//rxx/PS3J0eN/PyrJ85I8N8n1knwmyX2HYfjkBHMEAFaWzAeAfsh9AOiDzAeAeZbcKB+GoZawzRVJnjJeAIBVSOYDQD/kPgD0QeYDwLVtzyfKYeY2ffeSmYy78fu7zGTcQx7++ebai/9h7WSDb940WT0AXas7HTJR/XefckVz7YHr23P7ExNcde/DPzy4ufaSf96/uXafS09vrk2Svd58RnvtBONePUHtarXf2l2bay958o+ba/c9pbkUuvbVB+zeXLvv2g1TnAn0bd0BN22uffDeJ01xJttn9/Mvba71FxkA1t3kZyeqP+fur2+u3Ti0J9G5G5tL87UXH9hcu0fObB+Yrq2Z9QQAAAAAAAAAYCVplAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF1ZN+sJwGpw22d8sbn2UT93r+ba19/sQ821Rz3kj5prk+S6bztjonoAVr81GzY01179N9+faOwzDvrX5trzr/5Jc+1TnvXU5trrf+RrzbX77nFRc+2m5kpWk1+40Vebay+Y3jSgK+tu9YOZjHvledebybiwo/r6S/Zorr3rrpsnGvt1379Je/Flk70fBmD1W3vIbZprD3vL56Y4k5XzsH99YnPtLd+pJ8HK84lyAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRl3awnAKvBpssub6695A9v21z7tZOuaK790+e+sbk2SZ750GOaa4dP7dVcu//zTm+uzTC01wJwLVccdUhz7QcOeuUUZ7J9HvOkP26uve67zmiuvbq5EgB+at+zN896CuzE1t5gn+ba7/zGgc21ez/0G821px34uubaZLcJapN/OP7Xmmv3/c7HJhobgNXvqw9sz9137POpCUdf21z521/+1ebaA1/w5ebaTc2V0M4nygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAuqJRDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQlXWzngDs7DZ/5tzm2t989p801/7TX76ouTZJPn3EG9uLj2gvPWSPxzfX3vo1FzbXXv2VC5prAXZWP/+cTzfXrpnweMxHffVezbW7v+usicaGrVlfa5trNw7t466tCYqBVeWKvdszdI8pzmMlbb77oc21w9pqrv36vXdtrv3JjTc2167ZZVNz7Qfv/vLm2iRZ3/5w5dub2h+vP//KMc2139u8ubl2w5r2xzpJ9jvzB821khtg5/C9Rx3ZXPtvf/C3E4y8foLa5A++flRz7cZHtmf+pou/1lwLs+AT5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF1ZN+sJAIvb+4TTm2sf/4U/mmjsPV/wjebat97iA8215/zuK5prD9r/Mc21t3n2ZMcNbfrfr0xUD7BcLnvEkc21f7bfi5prN2eX5tok+cQHD26uvWk+NtHYsDUbh03NtZuzubn2/ee2f0/cOp9sroWeXXXl+ubazRmaa1//rL9vrj3p8Xdorp2lZ+zz2ubaNanm2iuGnzTXfmtTex684uKjm2vvffKTm2uT5Hqfan+PdqMPfqe5tr7a/jeGi8/dvbl2v7Ubm2uTZPj4ZyeqB2DHsPaQ2zTXfuy57X+vTnaboHYyp3/jgOba/S/43PQmAjs4nygHAAAAAAAAoCtLapRX1U2q6uVVdXpV/biqhqo6YIHthkWW1XlIMwB0RuYDQD/kPgD0QeYDwMKWeur1WyV5aJJPJPlIkvtsZdsTk/zjvHVf3O6ZAQCzIPMBoB9yHwD6IPMBYAFLbZT/1zAM+yVJVT0mWw/Sbw7DcMbEMwMAZkHmA0A/5D4A9EHmA8AClnTq9WEYNi/3RACA2ZP5ANAPuQ8AfZD5ALCwJTXKt9MfVtVV42udfLiq7r4MYwAAsyfzAaAfch8A+iDzAejGtBvlb07yuCT3TvLYJPsk+XBVHb1YQVU9tqrOrqqzN+aqKU8HAFgmMh8A+iH3AaAPMh+Ariz1GuVLMgzDI+b89yNV9e4kn0vy3CR3W6Tm1UlenSR71t7DNOcDACwPmQ8A/ZD7ANAHmQ9Ab5bj1Ov/ZxiGHyR5b5LDl3McAGC2ZD4A9EPuA0AfZD4AO7tlbZSPVRJHkgHAzk/mA0A/5D4A9EHmA7DTWtZGeVXtmeT+Sc5cznEAgNmS+QDQD7kPAH2Q+QDs7JZ8jfKqevD4n3caf71fVV2c5OJhGE6rqqcluU2SU5J8K8nNkjwtyQ2TPHx6UwYAlpPMB4B+yH0A6IPMB4BrW3KjPMnb5/3/leOvpyU5OskXkhwzXvZK8v0kH03ye8MwnDXZNAGAFSTzAaAfch8A+iDzAWCeJTfKh2Gobdz+niTvmXhGAMBMyXwA6IfcB4A+yHwAuLbt+UQ5sIrURz89Uf2PH7xvc+3hD3tCc+2Zz3hpc+1593xtc+3DD7hPc22SXH63icoBls3Vu7fX7rVml+ba06/ctX3gJLd447eaa6+eaGRWgzUbNjTXnvei2004+ieaKx/+lfs11x70pPObazc1V0LfbvU7n2quPeT5j2+u3f/wbzbXrlanXHRgc+3F/3GT5tp9ztnYXLvL+z/eXJu0j3tgzp5g3MlMkifffMZdmmsP3/X05tp//uHPNtcCsPP44rPaf4fcOKzO36hu+oL22mF604Ad3ppZTwAAAAAAAAAAVpJGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0JV1s54AsGPa9J2Lmmv3e1l77ZVPv7q5dkPt0lz7mgP+vbk2SR5wzJObazf825kTjQ2wI7pk03Umqr/6KxdMZyLssNZs2NBc+4UX/Fxz7XkPekVzbZL8x4/3aq791vG3aq697qVnNNcCK+/mzzx91lPoxo3ytVlPgSXYcI+LZzLun53yGxPVH5izpjQTACa1+ahDm2ufe9i7pjiTlfFLn/vNieqvc/bnpjQT2Ln5RDkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAAAAAF3RKAcAAAAAAACgKxrlAAAAAAAAAHRFoxwAAAAAAACArmiUAwAAAAAAANAVjXIAAAAAAAAAurJu1hMAlsfmu91hovovP2S35trb3eGC5toNtUtz7SRe/r1DJ6rf8O6zpzQTgJ3D0z76kInqD8wnpjQTltPmo9rz86KnXNFce+5hr2iuvddnH9ZcmyR73PcrzbXXzRkTjQ0AbJ+bvXuY9RQAmJLnnfjq5trbrZ9NHjztwns01+71W5dONPamiaqhHz5RDgAAAAAAAEBXNMoBAAAAAAAA6IpGOQAAAAAAAABd0SgHAAAAAAAAoCsa5QAAAAAAAAB0RaMcAAAAAAAAgK5olAMAAAAAAADQFY1yAAAAAAAAALqiUQ4AAAAAAABAVzTKAQAAAAAAAOiKRjkAAAAAAAAAXdEoBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICurJv1BGBnV4fdrrn2i0/cpbn2NXd9Q3Ntktxjt59MVD8LVw0bm2vP+N7NJxt884WT1QMsl2ovXTPBMZUvvdtb2wdOcnwOnKiepfvqXx3ZXPvO331xc+2B69vf59zxrEc21974mM831wIAADAbh+7S/jeKjcOmKc5k6U5//R2ba/e99GNTnAmwGJ8oBwAAAAAAAKArGuUAAAAAAAAAdEWjHAAAAAAAAICuaJQDAAAAAAAA0BWNcgAAAAAAAAC6olEOAAAAAAAAQFc0ygEAAAAAAADoikY5AAAAAADw/7d37zGW3uV9wL8PrBNDgGALFBMHsyZgLCiNKcZxEoO5JBAImEqBoKhpKkeIxm0MoYFWNvyBKMRqKpoLggSaUpAgSlSchE2bEDAXl8sabC4qGApJsA0bLjEsGMcGg3d//eOcpePZuZz3PWfmnNnf5yMdHc857zPz7KN35jvj51wAoCsW5QAAAAAAAAB0xaIcAAAAAAAAgK5YlAMAAAAAAADQFYtyAAAAAAAAALpiUQ4AAAAAAABAVyzKAQAAAAAAAOiKRTkAAAAAAAAAXdm37AZgt+w780Gja//u4h8eXfuy5/zx6Nqfv9dXR9fuVZd/5dzRtVf/7vmja09508HRtQArrY0vPZqjo2svvMfXxn/hJL/+xkePrv3R/z6+75O+fOvo2q9ceP/Rtac+59Do2kvPeNfo2iR56j0/Mrr2wG0/NLr2lz/xs6Nr7/e6HxhdCwDsLXev8c/z+fpZJ831tU/7q7nKAVjnC2/9J6NrT6qPL7CT3fGA947///tHFtgHsDnPKAcAAAAAAACgK9suyqvqWVV1ZVXdVFXfqqrPVNUVVXXvdcedUlV/WFVfrarbquqqqnrkzrUOACya3AeAPsh8AOiH3AeAjc3yjPIXZfIqD5cn+dkkv5/kkiTvrJq89lFVVZID0/svTfLzSU5K8p6q+pEd6BsA2BlyHwD6IPMBoB9yHwA2MMt7lD+jtXbzmo+vrqrDSd6U5PFJ3p3koiQXJHlia+09SVJVB5PckOTfJ3n+IpsGAHaM3AeAPsh8AOiH3AeADWz7jPJ1AXrMtdPr06fXFyX54rEAndbdkuQvkjxz3iYBgN0h9wGgDzIfAPoh9wFgY7O89PpGLpxef3p6/Ygkn9zguOuTnFFV9xr5dQCA5ZP7ANAHmQ8A/ZD7AHRv8KK8qk5P8vIkV7XWrpvefGqSr29w+OHp9SlbfL7nVdV1VXXdd3PH0HYAgB20yNyX+QCwuvytDwD98Lc+AEwMWpRPHzX2tiR3Jrl47V1J2kYl233O1trrW2vnttbOPSnfP6QdAGAHLTr3ZT4ArCZ/6wNAP/ytDwD/375ZD6yqk5McSPLgJBe21g6tuftwJo84W+/Yo8w2eiQaALCi5D4A9EHmA0A/5D4A3NVMzyivqpOSXJnkvCRPa619Yt0h12fyHibrPTzJ51tr/zhXlwDArpH7ANAHmQ8A/ZD7AHC8bRflVXW3JG9J8qQkz2ytXbPBYQeSnF5VF66pu0+SZ0zvAwD2ALkPAH2Q+QDQD7kPABub5aXXX5Pk2UlemeS2qjp/zX2Hpi/PciDJwSRvrqoXZ/IyLJdl8v4lv7XYlgGAHST3AaAPMh8A+iH3AWADs7z0+lOn1y/JJCjXXp6bJK21o0menuSdSV6b5M+SHEnyhNbaFxbcMwCwc+Q+APRB5gNAP+Q+AGxg22eUt9b2z/KJWmuHk/zK9AIA7EFyHwD6IPMBoB9yHwA2NstLr8PC7Nt/xlz1tzz6AaNrn/Pyt4+u/dX7/uno2r3qN750/vYHbeLga88dXXvqGz88uvaUowdH1wKwWCfXfL9mfvpn/mB07fsfe/Lo2r+547TRtRf/4I2ja5fpBV987Ojat3/wnNG1D33BRm+LCABwV0fa0fHFs7yWJgAzO3rho+aq/51z3jy69rvtyOjaW45+e3TtY/7q10fXnn3Tp0bXArvDr4sAAAAAAAAAdMWiHAAAAAAAAICuWJQDAAAAAAAA0BWLcgAAAAAAAAC6YlEOAAAAAAAAQFcsygEAAAAAAADoikU5AAAAAAAAAF2xKAcAAAAAAACgKxblAAAAAAAAAHTFohwAAAAAAACArliUAwAAAAAAANAVi3IAAAAAAAAAumJRDgAAAAAAAEBXLMoBAAAAAAAA6Mq+ZTfAcux7wGmjaw+/4QdG115y5tWja5PkF+/9lbnq95pf+/sLRtd+9PfPmetr3++tnxxde+qtB+f62gAszg+99x9G1/6Hf/0To2v/02nLy4LHnfyd0bUXnHzj4hoZ4GN3jH/86i9e/by5vvZZF39kdO1Dc81cXxsAYCfd/pjbl90CwAnl26d+31z1F5x82xzVdx9d+de3nzG69qznXTu69ujoSmC3eEY5AAAAAAAAAF2xKAcAAAAAAACgKxblAAAAAAAAAHTFohwAAAAAAACArliUAwAAAAAAANAVi3IAAAAAAAAAumJRDgAAAAAAAEBXLMoBAAAAAAAA6IpFOQAAAAAAAABdsSgHAAAAAAAAoCsW5QAAAAAAAAB0xaIcAAAAAAAAgK5YlAMAAAAAAADQFYtyAAAAAAAAALqyb9kN9O47Tzl3fO0LD4+uvfwhfzm69sn3uG107V71lSPfGl37uAO/Mbr27Jf+39G1p37j4OjaJDk6VzUAq+LIZ/9udO3fPHv/6NqHX3rp6Nok+dQvvHqu+mU4+y//zejah7329tG1Z33sI6NrAQBW3d3L83wAANgZftMEAAAAAAAAoCsW5QAAAAAAAAB0xaIcAAAAAAAAgK5YlAMAAAAAAADQFYtyAAAAAAAAALpiUQ4AAAAAAABAVyzKAQAAAAAAAOiKRTkAAAAAAAAAXbEoBwAAAAAAAKArFuUAAAAAAAAAdMWiHAAAAAAAAICuWJQDAAAAAAAA0BWLcgAAAAAAAAC6YlEOAAAAAAAAQFf2LbuB3t34z8c/VuGzj/wfC+xkd7zmGz86V/3vXv3k0bV1pEbXnv2KG0bXPvQrHxpde2R0JQDM787P3Ti69iEvHF+bJBe98DFz1S/DWbl2dG1bYB8AAKvmjqvuP7r2yDlHF9gJAPO4z8e/PFf9pYeeOLr2Dx549VxfG2AjnlEOAAAAAAAAQFcsygEAAAAAAADoikU5AAAAAAAAAF2xKAcAAAAAAACgKxblAAAAAAAAAHTFohwAAAAAAACArliUAwAAAAAAANAVi3IAAAAAAAAAumJRDgAAAAAAAEBXLMoBAAAAAAAA6IpFOQAAAAAAAABdsSgHAAAAAAAAoCsW5QAAAAAAAAB0xaIcAAAAAAAAgK7sW3YDvTvrkg+Prn36JY9eYCd7w1kZP695HFnKVwUAAADY+0777Q+Orn3ab/+z0bUPzsdH1wJwvDtvuGmu+kPnj699evrbhwA7zzPKAQAAAAAAAOiKRTkAAAAAAAAAXdl2UV5Vz6qqK6vqpqr6VlV9pqquqKp7rzlmf1W1TS733dl/AgCwKHIfAPog8wGgH3IfADY2y3uUvyjJ55NcnuRQkkcleVmSJ1TVT7bWjq459ookB9bV37qAPgGA3SH3AaAPMh8A+iH3AWADsyzKn9Fau3nNx1dX1eEkb0ry+CTvXnPf51pr1yywPwBgd8l9AOiDzAeAfsh9ANjAti+9vi5Aj7l2en36YtsBAJZJ7gNAH2Q+APRD7gPAxrZdlG/iwun1p9fdfkVV3VlVt1TVgap65By9AQCrQe4DQB9kPgD0Q+4D0L1ZXnr9Lqrq9CQvT3JVa+266c13JHldknckuTnJ2Zm838kHq+q81tr6sF37+Z6X5HlJcnLuObQdAGAHLTL3ZT4ArC5/6wNAP/ytDwAT1Vqb/eCqeyV5b5IfTnJea+3QFsc+MMn1SQ601n5pls9/nzq1/Xg9aeZ+AGA3fKi9K99sh2vZfey2ncx9mQ/AqrqqvfUjrbVzl93HbvK3PgA98re+v/UB6Mdmf+vP/Izyqjo5yYEkD05y4VYBmiSttS9U1fuTPGZoswDAcsl9AOiDzAeAfsh9ALirmRblVXVSkiuTnJfkp1trn5jx81eS2Z+yDgAsndwHgD7IfADoh9wHgOPdbbsDqupuSd6S5ElJntlau2aWT1xVZyT5qSQfmqtDAGDXyH0A6IPMB4B+yH0A2Ngszyh/TZJnJ3llktuq6vw19x1qrR2qqldlsnQ/mOTmJA9LclmSo0l+c7EtAwA7SO4DQB9kPgD0Q+4DwAa2fUZ5kqdOr1+SSUiuvTx3et/1SS5I8rok70zysiQfSPLjrbXPLLBfAGBnyX0A6IPMB4B+yH0A2MC2zyhvre2f4Zg3JHnDIhoCAJZH7gNAH2Q+APRD7gPAxmZ5RjkAAAAAAAAAnDAsygEAAAAAAADoikU5AAAAAAAAAF2xKAcAAAAAAACgKxblAAAAAAAAAHTFohwAAAAAAACArliUAwAAAAAAANAVi3IAAAAAAAAAumJRDgAAAAAAAEBXLMoBAAAAAAAA6IpFOQAAAAAAAABdsSgHAAAAAAAAoCsW5QAAAAAAAAB0xaIcAAAAAAAAgK5YlAMAAAAAAADQFYtyAAAAAAAAALpiUQ4AAAAAAABAVyzKAQAAAAAAAOiKRTkAAAAAAAAAXbEoBwAAAAAAAKArFuUAAAAAAAAAdMWiHAAAAAAAAICuWJQDAAAAAAAA0BWLcgAAAAAAAAC6YlEOAAAAAAAAQFcsygEAAAAAAADoikU5AAAAAAAAAF2xKAcAAAAAAACgKxblAAAAAAAAAHTFohwAAAAAAACArlRrbdk9fE9V3Zzkpk3uvl+Sr+5iO3udeQ1jXsOY1+zMaphVndeDWmv3X3YTJ5JtMj9Z3XNhFZnVMOY1jHkNY17DrOq85P6C+Vt/ocxrGPMaxrxmZ1bDrOq8ZP6C+Vt/ocxqGPMaxryGMa9hVnVeG+b+Si3Kt1JV17XWzl12H3uFeQ1jXsOY1+zMahjz4hjnwuzMahjzGsa8hjGvYcyLxHkwlHkNY17DmNfszGoY8+IY58LszGoY8xrGvIYxr2H22ry89DoAAAAAAAAAXbEoBwAAAAAAAKAre2lR/vplN7DHmNcw5jWMec3OrIYxL45xLszOrIYxr2HMaxjzGsa8SJwHQ5nXMOY1jHnNzqyGMS+OcS7MzqyGMa9hzGsY8xpmT81rz7xHOQAAAAAAAAAswl56RjkAAAAAAAAAzM2iHAAAAAAAAICurPSivKoeWFVvrapbquqbVfWnVXXGsvtaRVX1+KpqG1y+sezelq2qfqSqXl1VB6vq9ulc9m9w3MlV9Z+r6ktV9a3p8Y/b/Y6Xa8C8NjrfWlWds/tdL0dVPauqrqyqm6bnzGeq6oqquve6406pqj+sqq9W1W1VdVVVPXJZfS/LLPOqqv1bnFv3XWb/7Dy5Pzu5vzm5P4zcn53cH0busxWZPzuZvzmZP4zMn53MH0bmsx25Pzu5vzm5P4zcn53cH+ZEzP19y25gM1V1zyTvTnJHkn+VpCV5RZL3VNU/ba3dtsz+Vtjzk1y75uM7l9XICnlIkl9I8pEk70vy5E2O+29Jfi7Ji5N8Lsm/TfLXVfUTrbWP70ajK2LWeSXJG5O8bt1tn92ZtlbSi5J8PsnlSQ4leVSSlyV5QlX9ZGvtaFVVkgNJzkxyaZKvJ7ksk59l57TWDi2l8+XYdl5rjr0ik7mtdetuNMlyyP3R5P7x5P4wcn92cn8Yuc+GZP5oMv94Mn8YmT87mT+MzGdTcn80uX88uT+M3J+d3B/mxMv91tpKXpK8IMmRJA9Zc9uZmYTCv1t2f6t2SfL4TH7R+Oll97JqlyR3W/Pfz53Oaf+6Y35sevvFa27bl+QzSQ4s+9+wavOa3teSvGLZ/S55Vvff4LZfns7midOPNOHLHwAABtJJREFUnzn9+AlrjvnBJIeT/N6y/w0rOK/904+fu+x+XXb9/JD7w+Yl9zefjdxf8Lym98l9ub8T85L7HV5k/uB5yfzNZyPzFzyv6X0yX+bvxLxkfqcXuT94XnJ/89nI/QXPa3qf3Jf7OzGvPZX7q/zS6xcluaa19rfHbmit3ZDkA5mclDCTdtdHsGzmoiTfTfIna+ruTPLHSZ5SVd+/Q+2tnBnnRZLW2s0b3Hzs0Z6nT68vSvLF1tp71tTdkuQv0tnPshnnRb/kPgsh94eR+7OT+8PIfbYg81kImT+MzJ+dzB9G5rMNuc9CyP1h5P7s5P4wJ2Lur/Ki/BFJPrnB7dcnefgu97KXvKWqjlTV16rqj7zfy8wekeSG1trt626/Psn3ZfJSJRzvkqq6Y/o+J++uqscuu6EVcOH0+tPT661+lp1RVffala5W1/p5HXNFVd05ff+qAz2+30uH5P44cn8cuT+O3D+e3B9G7pPI/LFk/jgyfxyZfzyZP4zM5xi5P47cH0fujyP3jyf3h9nTub+y71Ge5NRMXud/vcNJTtnlXvaCW5K8KsnVSb6ZyfsCXJ7kYFU9qrX2D8tsbg/Y6nw7dj939eYk/zPJF5M8KJP3fXl3Vf1Ma+29y2xsWarq9CQvT3JVa+266c2nJrlxg8OPnVunJPnHne9u9WwyrzsyeU+cdyS5OcnZmfws+2BVnddaWx+2nDjk/jByfz5yfzi5v47cH0bus4bMH0bmz0fmDyfz15H5w8h81pH7w8j9+cj94eT+OnJ/mBMh91d5UZ5MXsN+vdr1LvaA1trHknxszU1XV9X/TvLhJM9P8tKlNLZ3VJxvg7TW/uWaD99XVW/L5FFVr0hywXK6Wp7po8belsl7LF289q44t46z2bxaa19K8qtrDn1fVb09k0fnvSTJL+1mn+w63yszkvtz87N5ILl/V3J/GLnPBnyfzEjmz83P5YFk/l3J/GFkPpvwvTIjuT83P5sHkvt3JfeHOVFyf5Vfev3r2fgRPqdk40cFsU5r7aNJPpvkMcvuZQ84nM3Pt2P3s4XW2q1J/lc6PN+q6uQkB5I8OMlTWmuH1ty93bnV3c+zbeZ1nNbaF5K8Px2eW52R+3OS+4PI/TnJfbk/K7nPBmT+nGT+IDJ/TjJf5s9K5rMJuT8nuT+I3J+T3Jf7szqRcn+VF+XXZ/K6/+s9PMmndrmXvWyzR7pwV9cnObOq7rnu9ocn+U6Sv939lvak7s63qjopyZVJzkvytNbaJ9YdstXPss+31rp6SZYZ5rVpaTo7tzok9xfD98ps5P5idHe+yf1h5D6bkPmL4ftkNjJ/Mbo732T+MDKfLcj9xfC9Mhu5vxjdnW9yf5gTLfdXeVF+IMn5VfXgYzdU1f4kPzW9j21U1blJzkryoWX3sgccSHJSkmcfu6Gq9iV5TpJ3tNbuWFZje0VV3SfJz6Wj862q7pbkLUmelOSZrbVrNjjsQJLTq+rCNXX3SfKMdPazbMZ5bVR3RiY/+7s5tzol9+ck9weR+3OS+3J/O3KfLcj8Ocn8QWT+nGS+zN+OzGcbcn9Ocn8QuT8nuS/3t3Mi5v4qv0f5f03ya0neVlUvzeRRBv8xyRcyeRN41qiqtyS5IclHk3wjyaOSXJbk75O8eomtrYSqetb0Px89vX5qVd2c5ObW2tWttY9X1Z8k+Z3po2FuSHJJkjOT/Ivd73i5tptXVb0oycOSvCfJF5M8KMmLkpyWvub1mkx+8Xplktuq6vw19x2avtzIgSQHk7y5ql6cycuwXJbJo6d+a5f7XbZt51VVr8rkQVwHk9ycyXl2WZKjSX5zl/tld8n9AeT+1uT+MHJ/ZnJ/GLnPZmT+ADJ/azJ/GJk/M5k/jMxnK3J/ALm/Nbk/jNyfmdwf5sTL/dbayl6SnJHJ0/e/meTWJH+eZP+y+1rFSyYn2f9JckuS72byy8brkzxg2b2twiWTX8I2urx3zTH3SPJfknw5ybczeWTL45fd+yrOK5NHSn0gyVen59vXMgmL85bd+y7P6cYtZvWyNcedmuQNmbyXye1J3pXkx5bd/yrOK8mvJLk2k1827px+P/5Rkoctu3+XXTlH5P7ss5L7W89H7i9wXnL/e3OS+wuel9zv9yLzB81K5m89H5m/wHnJ/O/NSeYveF4yv++L3B80K7m/9Xzk/gLnJfe/Nye5v+B57bXcr2nTAAAAAAAAANCFVX6PcgAAAAAAAABYOItyAAAAAAAAALpiUQ4AAAAAAABAVyzKAQAAAAAAAOiKRTkAAAAAAAAAXbEoBwAAAAAAAKArFuUAAAAAAAAAdMWiHAAAAAAAAICu/D+oZTCJYwNdZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2520x2520 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise data\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, axes = plt.subplots(1,4, figsize=(35,35))\n",
    "imx, imy = (28,28)\n",
    "labels   = [0,1,2,3]\n",
    "for i, ax in enumerate(axes):\n",
    "    visual = np.reshape(x_train[labels[i]], (imx,imy))\n",
    "    ax.set_title(\"Example Data Image, y=\"+str(int(y_train[labels[i]])))\n",
    "    ax.imshow(visual, vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVIB(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, z_dim):\n",
    "        \"\"\"\n",
    "        Deep VIB Model.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        input_shape : `int`\n",
    "            Flattened size of image. (Default=784)\n",
    "        output_shape : `int`\n",
    "            Number of classes. (Default=10)            \n",
    "        z_dim : `int`\n",
    "            The dimension of the latent variable z. (Default=256)\n",
    "        \"\"\"        \n",
    "        super(DeepVIB, self).__init__()\n",
    "        self.input_shape  = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.z_dim  = z_dim\n",
    "        \n",
    "        # build encoder\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_shape, 1024),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(1024, 1024),\n",
    "                                     nn.ReLU(inplace=True)                                    )  \n",
    "        self.fc_mu  = nn.Linear(1024, self.z_dim) \n",
    "        self.fc_std = nn.Linear(1024, self.z_dim)\n",
    "        \n",
    "        # build decoder\n",
    "        self.decoder = nn.Linear(self.z_dim, output_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        x : [batch_size,784]\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), F.softplus(self.fc_std(x)-5, beta=1)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        z : [batch_size,z_dim]\n",
    "        \"\"\" \n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterise(self, mu, std):\n",
    "        \"\"\"\n",
    "        mu : [batch_size,z_dim]\n",
    "        std : [batch_size,z_dim]        \n",
    "        \"\"\"        \n",
    "        # get epsilon from standard normal\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std*eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : [batch_size,28,28]\n",
    "        \"\"\"\n",
    "        # flattent image\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        mu, std = self.encode(x_flat)\n",
    "        z = self.reparameterise(mu, std)\n",
    "        return self.decode(z), mu, std     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "z_dim  = 256\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "decay_rate = 0.97\n",
    "beta = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "DeepVIB(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc_std): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (decoder): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create DatatLoader \n",
    "train_dataset    = data_utils.TensorDataset(x_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "\n",
    "# Loss function: Cross Entropy Loss (CE) + beta*KL divergence\n",
    "def loss_function(y_pred, y, mu, std):\n",
    "    \"\"\"    \n",
    "    y_pred : [batch_size,10]\n",
    "    y : [batch_size,10]    \n",
    "    mu : [batch_size,z_dim]  \n",
    "    std: [batch_size,z_dim] \n",
    "    \"\"\"   \n",
    "    CE = F.cross_entropy(y_pred, y, reduction='sum')\n",
    "    KL  = 0.5 * torch.sum(mu.pow(2) + std.pow(2) - 2*std.log() - 1)\n",
    "    return (beta*KL + CE) / y.size(0)\n",
    "\n",
    "# Initialize Deep VIB\n",
    "vib = DeepVIB(np.prod(x_train[0].shape), n_classes, z_dim)\n",
    "\n",
    "# Optimisers\n",
    "optimiser = torch.optim.Adam(vib.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimiser, gamma=decay_rate)\n",
    "\n",
    "# Send to GPU if available\n",
    "vib.to(device)\n",
    "\n",
    "print(\"Device: \", device)\n",
    "print(vib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200... Loss: 0.8316... Time Taken: 15.4374 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-d7cc0a7430b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Update parameters of generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Save loss per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "measures = defaultdict(list)\n",
    "start_time = time.time()\n",
    "\n",
    "# put Deep VIB into train mode \n",
    "vib.train()  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()  \n",
    "    \n",
    "    # exponential decay of learning rate every 2 epochs\n",
    "    if epoch % 2 == 0 and epoch > 0:\n",
    "        scheduler.step()    \n",
    "    \n",
    "    batch_loss = 0\n",
    "    for _, (X,y) in enumerate(train_dataloader): \n",
    "        X = X.to(device)        \n",
    "        y = y.long().to(device)\n",
    "        \n",
    "        # Zero accumulated gradients\n",
    "        vib.zero_grad()\n",
    "        \n",
    "        # forward pass through Deep VIB\n",
    "        y_pred, mu, std = vib(X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(y_pred, y, mu, std)\n",
    "        # Backpropogation: calculating gradients\n",
    "        loss.backward()\n",
    "        # Update parameters of generator\n",
    "        optimiser.step()  \n",
    "        \n",
    "        # Save loss per batch\n",
    "        batch_loss += loss.item()*X.size(0) \n",
    "           \n",
    "    # Save losses per epoch\n",
    "    measures['total_loss'].append(batch_loss / len(train_dataloader.dataset))        \n",
    "    \n",
    "    print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "          \"Loss: {:.4f}...\".format(measures['total_loss'][-1]),\n",
    "          \"Time Taken: {:,.4f} seconds\".format(time.time()-epoch_start_time))\n",
    "    \n",
    "print(\"Total Time Taken: {:,.4f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9371... Time Taken: 0.4544 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create DatatLoader \n",
    "test_dataset = data_utils.TensorDataset(x_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "\n",
    "measures = defaultdict(int)\n",
    "start_time = time.time()\n",
    "    \n",
    "# put Deep VIB into train mode \n",
    "vib.eval()       \n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, (X,y) in enumerate(test_dataloader): \n",
    "        X = X.to(device)        \n",
    "        y = y.long().to(device)\n",
    "        \n",
    "        # forward pass through Deep VIB\n",
    "        y_pred, mu, std = vib(X)\n",
    "    \n",
    "        y_pred = torch.argmax(y_pred,dim=1)\n",
    "        measures['accuracy'] += int(torch.sum(y == y_pred))\n",
    "\n",
    "print(\"Accuracy: {:.4f}...\".format(measures['accuracy']/len(test_dataloader.dataset)),\n",
    "      \"Time Taken: {:,.4f} seconds\".format(time.time()-start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
